{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define a list of models or algorithms to compare\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('SVM', SVC()),\n",
    "    ('Perceptron', MLPClassifier(hidden_layer_sizes=(100, ))),\n",
    "]\n",
    "\n",
    "# Define a dictionary of hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'Perceptron': {\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'activation': ['logistic', 'relu'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Outer loop for cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Initialize a dictionary to store the evaluation scores for each model\n",
    "model_scores = {}\n",
    "\n",
    "# Outer loop for cross-validation\n",
    "for train_index, test_index in outer_cv.split(X):\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Inner loop for model selection and hyperparameter tuning\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    for model_name, model in models:\n",
    "\n",
    "        # Define the hyperparameter grid for the current model\n",
    "        param_grid = param_grids[model_name]\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best model from the inner loop\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluate the best model on the testing set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        score = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Store the evaluation score for the current model\n",
    "        if model_name in model_scores:\n",
    "            model_scores[model_name].append(score)\n",
    "        else:\n",
    "            model_scores[model_name] = [score]\n",
    "\n",
    "# Calculate the mean and standard deviation of the evaluation scores for each model\n",
    "mean_scores = {model_name: np.mean(scores) for model_name, scores in model_scores.items()}\n",
    "std_scores = {model_name: np.std(scores) for model_name, scores in model_scores.items()}\n",
    "\n",
    "\n",
    "# Print the results for each model\n",
    "for model_name, mean_score in mean_scores.items():\n",
    "    std_score = std_scores[model_name]\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Mean Accuracy:\", mean_score)\n",
    "    print(\"Standard Deviation:\", std_score)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
